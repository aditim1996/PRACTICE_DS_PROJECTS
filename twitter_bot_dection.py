# -*- coding: utf-8 -*-
"""Twitter_bot_dection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YnN2oNT_7qFi0PgLridNe5qKEhsU9P9i
"""

import pandas as pd

#importing data
df = pd.read_csv('bot_detection_data.csv')
df.head()

df.info()

df.isna().sum()

## Replacing the categorical nan values with unknown ##

df['Hashtags'] = df['Hashtags'].fillna(value='Unknown')
df.head()

df['Created At'] = pd.to_datetime(df['Created At'], format='%Y-%m-%d %H:%M:%S')

df.info()

df.describe()

##

col_drop = ['User ID','Retweet Count','Mention Count','Follower Count','Verified','Bot Label','Hashtags']

data_dropped = df.drop(columns=col_drop)

## getting the count for each column

column_counts = {}
for column in data_dropped.columns:
  column_counts[column] = data_dropped[column].value_counts()

for column, counts in column_counts.items():
    print(f"Value counts for {column}:")
    print(counts)
    print()

df['Hashtags'].value_counts()

### Dividing bot and non bot data to deep dive

bot_data = df[df['Bot Label']== 1]
non_bot_data = df[df['Bot Label']== 0]

### Looking into statistics for bot data & non bot data
# 'Retweet Count','Mention Count','Follower Count','Verified','Bot Label','Hashtags'

bot_retweet_stats = bot_data['Retweet Count'].describe()
bot_retweet_stats

non_bot_retweet_stats = non_bot_data['Retweet Count'].describe()
non_bot_retweet_stats

bot_follower_stats = bot_data['Follower Count'].describe()
bot_follower_stats

non_bot_follower_stats = non_bot_data['Follower Count'].describe()
non_bot_follower_stats

bot_mention_stats = bot_data['Mention Count'].describe()
bot_mention_stats

non_bot_mention_stats = non_bot_data['Mention Count'].describe()
non_bot_mention_stats

print("Follower Count Statistics:")
print("Bot Accounts:")
print(bot_follower_stats)
print("\nNon-Bot Accounts:")
print(non_bot_follower_stats)

print("\nRetweet Count Statistics:")
print("Bot Accounts:")
print(bot_retweet_stats)
print("\nNon-Bot Accounts:")
print(non_bot_retweet_stats)

print("\nMention Count Statistics:")
print("Bot Accounts:")
print(bot_mention_stats)
print("\nNon-Bot Accounts:")
print(non_bot_mention_stats)

#label encoding of boolean data
from sklearn.preprocessing import LabelEncoder

label_encoder=LabelEncoder()
df['Verified']=label_encoder.fit_transform(df['Verified'])
df.head()

## Processing text features in data frame

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack

text_data = df['Tweet'] + ' ' + df['Username'] + ' ' + ['Hashtags']+ ' ' + df['Location']

# Vectorization using Tf-IDF (sparsing matrix from text data)

vectorizer = TfidfVectorizer()
text_sparse = vectorizer.fit_transform(text_data)

### Combining both text data and other numerical features

added_features = df[['Retweet Count', 'Verified','Mention Count','Follower Count','Created At']]

added_features ['Created At'] = added_features['Created At'].astype(int)
added_features = added_features.astype('float64')
text_sparse = text_sparse.astype('float')

#Combine data

combine_data = hstack((text_sparse,added_features))

## Fiting the Random forest model

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(combine_data, df['Bot Label'], test_size=0.2, random_state=42)

# Create an instance of the Random Forest classifier
rf_classifier = RandomForestClassifier()

# Train the Random Forest classifier
rf_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

# Example: Predict labels for new data
from datetime import datetime



# New data (example)

new_data = pd.DataFrame({

    'Tweet': ['Just cover eight opportunity strong policy which.'],

    'Username': ['pmason'],

    'Hashtags': ['neever quickly new Iw'],

    'Retweet Count': [54],

    'Verified': [1],

    'Location' : ['Martinezberg'],

    'Mention Count' : [5],

    'Follower Count' : [2242],

    'Created At' : ['14-08-2021  22:27:00']
})




# Concatenate text features

new_text_data = new_data['Tweet'] + ' ' + new_data['Username'] + ' ' + new_data['Hashtags']+' '+new_data['Location']




# Perform feature vectorization on new text features

new_text_sparse = vectorizer.transform(new_text_data)




# Combine new text features with additional features

new_additional_features = new_data[['Retweet Count', 'Verified','Mention Count','Follower Count','Created At']]



new_additional_features['Created At'] = pd.to_datetime(new_additional_features['Created At'])
new_additional_features['Created At'] = new_additional_features['Created At'].apply(lambda x: int(datetime.timestamp(x)))

# new_additional_features = new_additional_features.astype('float64')


# new_additional_features['Created At'] = new_additional_features['Created At'].astype(int)  # Convert to Unix timestamp
new_additional_features = new_additional_features.astype('float64')

new_combined_sparse = hstack((new_text_sparse, new_additional_features))




# Predict labels for the new data

new_predictions = rf_classifier.predict(new_combined_sparse)




# Print the predictions

for i, prediction in enumerate(new_predictions):

    if prediction == 1:

        print(f"Data point {i+1}: Bot")

    else:

        print(f"Data point {i+1}: Not Bot")

